{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b661d4",
   "metadata": {},
   "source": [
    "**Lecutre 3 - Backpropogation, Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c8c04",
   "metadata": {},
   "source": [
    "- non-linearities (activation functions)\n",
    "    - logistic/sigmoid\n",
    "    - tanh\n",
    "    - hard tan h\n",
    "    - ReLu (rectified linear unit): ReLU(z) = max(z,0)\n",
    "        - a lot is dead but some things are alive\n",
    "        - trains quickly and performs will due to good gradient backflow\n",
    "    - leaky ReLu / parametric ReLu\n",
    "        - z<0 is not 0 but slightly sloped < 0\n",
    "\n",
    "- why non-linearities are needed?\n",
    "    - neural networks do function approximation (regression/classification)\n",
    "    - without non-linearities, deep NN can't do anything more than a linear transform\n",
    "    - extra layers could collapse to single linear transform: W_1 * W_2 * x = Wx\n",
    "    - but with more layers that include non-linearities, they can approximate any complex function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97c62f",
   "metadata": {},
   "source": [
    "**Back propogation**\n",
    "\n",
    "- how to we calculate the slope of the loss function\n",
    "\n",
    "    - 1. by hand (matrix calculus)\n",
    "        - fully vectorized gradients\n",
    "        - gradient = vector of partial derivatices with respect to each input\n",
    "        - given a function with m outputs and n inputs\n",
    "            - its Jacobian is an m x n matrix of partial derivatives\n",
    "        - for composition of one-variable functions: multiply derivatives (dz/dx = dz/dy * dy/dx)\n",
    "            - for multiple variables functions: multiple Jacobians\n",
    "                - h = f(z)\n",
    "                - z = Wx + b\n",
    "                - dh/dx = dh/dz * dz/dx\n",
    "        - apply the chain rule\n",
    "            - s = u^transpose * h\n",
    "            - h = f(z)\n",
    "            - z = Wx + b\n",
    "            - x (input)\n",
    "            - ds/db = ds/dh * dh/dz * dz/db\n",
    "            - ds/dW = ds/dh * dh/dz * dz/dW\n",
    "            - note: the first 2 elements are the same...let's avoid duplicated computation and set those = _delta_\n",
    "                - _delta_ is the upstream gradient (\"error signal\")\n",
    "        - what's the output shape of ds/dW?\n",
    "            - 1 output, nm inputs = 1 by nm Jacobian\n",
    "            - computationally expensive\n",
    "            - leave pure math and use the **shape convention:** the shape of the gradient is the shape of the parameters!\n",
    "                - so n x m matrix\n",
    "        - derivative with respect to matrix\n",
    "            - ds/dW = _delta_ * dx/dW\n",
    "            - ds/dW = _delta_^transpose * x^transpose\n",
    "                - _delta_ = upstream gradient/error signal at z\n",
    "                - x is local input signal\n",
    "            - why the transpose?\n",
    "                - each input goes to each output - you want to get outer product\n",
    "                - let's consider the derivative of a single weight Wij\n",
    "                - Wij only contributes to z_i (i.e. W_23 is only used to compute z2 not z1)\n",
    "        - what shape should derivatives be?\n",
    "            - disagreement between Jacobian form (which makes chain rule easy) and the shape convention (which makes implementing SGD easy) (Jacobian results in row vector but shape convention says gradient should be a column vector bc b is a column vector)\n",
    "            - expect hw answers to follow shape convention\n",
    "            - but Jacobian form is useful for computing the answers\n",
    "            - 2 options:\n",
    "                - 1. use Jacobian as much as possible and reshape to follow shape convention at the end\n",
    "                - 2. always follow the shape convention\n",
    "                    - look at dimensions to figure out when to transpose and/or reorder terms\n",
    "                    - error message _delta_ that arrives at a hidden layer has the same dim as that hidden layer\n",
    "\n",
    "    - 2. **back propogation** algorithm\n",
    "        -  one more concept: we re-use derivatives computed for higher layers in computing derivatives for lower layers to minimize computation\n",
    "        - forward propogation\n",
    "            - x --> W\n",
    "            - then Wx --> b \n",
    "            - then z = Wx + b --> f\n",
    "            - then h = f(z) --> u\n",
    "            - then s --> output\n",
    "        - backward propogation\n",
    "            - go backwards along edges, pass along gradients\n",
    "            - ds/ds --> ds/dh --> ds/dz --> ds/db\n",
    "            - single node:\n",
    "                - node receives an upstream gradient (node f receives ds/dh coming from h)\n",
    "                - goal is to pass on the correct downstream gradient (node f passes ds/dz to z)\n",
    "                    - ds/dz = ds/dh * dh/dz (chain rule)\n",
    "            - node intuitions (forward propogation):\n",
    "                - + distributes upstream gradient\n",
    "                - max() routes upstream gradient\n",
    "                - * switches the forward coefficients in the downstream gradient\n",
    "            - efficiently compute\n",
    "                - compute all gradients at once\n",
    "        - **automatic differentiation**\n",
    "            - inferred from the symbolic expression of the fprop\n",
    "            - each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output\n",
    "            - modern DL frameworks (Tensorflow, PyTorch) do backpropogation for you but mainly leave layer/node writer to hand-calculate the local derivatice\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a35bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop implementation\n",
    "\n",
    "class ComputationalGraph(object):\n",
    "    \n",
    "    def forward(inputs):\n",
    "        for gate in self.graph.nodes_topologically_sorted()\n",
    "            gate.forward()\n",
    "        retrun loss\n",
    "    \n",
    "    def backward():\n",
    "        for gate in reversed(self.graph.nodes_topologically_sorted()):\n",
    "            gate.backward()\n",
    "        return inputs_gradients\n",
    "    \n",
    "# forward/backward API\n",
    "\n",
    "class MultiplyGate(object):\n",
    "\n",
    "    def forward(x,y):\n",
    "        z = x*y\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return z\n",
    "    \n",
    "    def backward(dz):\n",
    "        dx = self.y * dz # [dz/dx * dL/dz]\n",
    "        dy = self.x * dz # [dz/dy * dL/dz]\n",
    "        return [dx, dy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c2375",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- backpropagation = recursively apply the chain rule along computation graph\n",
    "    - downstream gradient = upstream gradient x local gradient\n",
    "- forward pass = compute results of operations and save intermediate values\n",
    "- backward pass = apply chain rule to compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e41b9d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
