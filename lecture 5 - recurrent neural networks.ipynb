{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292fc57f",
   "metadata": {},
   "source": [
    "**Lecture 5 - Recurrent Neural Networks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bf3e2",
   "metadata": {},
   "source": [
    "- Modern neural networks are enormous (over 100 billion parameters)\n",
    "\t- We need regularization to prevent neural networks from overfitting\n",
    "\t\t- Now regularization produces models that generalize well\n",
    "\t\t- But we do not care that we overfit training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae29da7",
   "metadata": {},
   "source": [
    "**Dropout**\n",
    "- When training, for each data point each time, randomly set input to 0 with probability p (dropout ratio, often p = 0.5 except p = 0.15 for input layer) via dropout mask\n",
    "- Why it works\n",
    "\t-Knows that some features are missing, so has be flexible in learning\n",
    "    - Prevents feature co-adaptation = good regularization\n",
    "- In a single layer - middle ground between NaÃ¯ve Bayes (all feature weights set independently) and logistic regression models (weights are set in the context of all others)\n",
    "    - A form of model bagging like an ensemble model\n",
    "    - Usually thought of as strong, feature-dependent regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391339e",
   "metadata": {},
   "source": [
    "**Vectorization**\n",
    "- Looping over word vectors vs. concatenating them all into one large matrix and then multiplying the softmax weights with that matrix\n",
    "- Always try to use vectors and matrices instead of for loops\n",
    "- Use vector operations for your mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "    N = 500 # number of windows to classify\n",
    "    d = 300 # dimensionality of each window\n",
    "    c = 5 # number of classes\n",
    "    w = random.rand(C,d)\n",
    "    wordvectors_list = [random.rand(d,1) for I in range(N)]\n",
    "    wordvectors_one_matrix = random.rand(d,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76428929",
   "metadata": {},
   "source": [
    "**Parameter initialization**\n",
    "- Must initialize weights to small random values (not zero matrices)\n",
    "- To avoid symmetries that prevent learning/specialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac26e9",
   "metadata": {},
   "source": [
    "**Optimizers**\n",
    "- Getting SGD rights is very dependent on getting the scales right for step size and learning rate\n",
    "- For more complex nets, try more sophisticated \"adaptive\" optimizers that scale the adjustment to individual parameters by an accumulated gradient\n",
    "    - These models give differential per-parameter learning rates\n",
    "        - Adagrad - simplest, but tends to stall early\n",
    "        - RMSprop\n",
    "        - Adam - usually being with this\n",
    "        - AdamW\n",
    "        - NAdamW - can be better with word vectors (W) and for speed (Nesterov acceleration)\n",
    "    - Start them with an initial learning rate, around 0.001 - many have other hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a836e5e",
   "metadata": {},
   "source": [
    "**Language modeling**\n",
    "- The task of predicting the next word\n",
    "- Given a sequence of words, compute the probability distribution of the next word, where next word can be any word in the vocabulary V\n",
    "- A system that assigns a probability to a piece of text\n",
    "\n",
    "**n-gram Language Models**\n",
    "- How to learn a language model\n",
    "\t- Pre-deep learning - learn an n-gram language model\n",
    "\t\t- n-gram = chunk of n consecutive words (we are trying to predict)\n",
    "\t\t- Unigram, bigram, trigram, four-gram\n",
    "\t\t- Idea: collect statistics about how frequent different n-grams are and use these to predict next word\n",
    "\t\t- we make a Markov assumption: x_t+1 depends only on the preceding n-1 words\n",
    "\t\t\t- Get n-gram and (n-1)-gram probabilities by counting them in some large corpus of text (statistical approximation)\n",
    "- Ex. Learning a 4-gram Language Model\n",
    "\t- \"students opened their ____\"\n",
    "\t- P(w | students opened their) = count(students opened their w) / count(students opened their)\n",
    "- Sparsity problems\n",
    "\t- Problem 1: what if \"students opened their w\" never occurred in data? Then w has probability 0.\n",
    "\t\t- Solution - add small _delta_ to the count for every w in V (\"smoothing\")\n",
    "\t- Problem 2: what if \"students opened their\" never occurred in data? Then can't calculate probability for any w\n",
    "\t\t- Solution - just condition on \"opened their\" instead (\"backoff\")\n",
    "\t- 5-grams are the largest people really use\n",
    "- Storage problems\n",
    "\t- Need to store count for all n-grams you saw in the corpus\n",
    "\t- Increasing n or increasing corpus increases model size\n",
    "- In practice\n",
    "\t- You can built a simple trigram language model over a 1.7 million word corpus (Reuters) in a few seconds on your laptop (nlpforhackers.io/language-models/\n",
    "- Generating text\n",
    "\t- Incoherent. We need to consider more than 3 words at a time in we want to model language well\n",
    "\t- But increasing n worsens sparsity problem, and increases model size\n",
    "\t\t- Solution - neural language model\n",
    "\n",
    "**Neural Language Models**\n",
    "- How to build a neural language model?\n",
    "\t- Window-based neural model\n",
    "\t\t- Output distribution\n",
    "\t\t- Hidden layer\n",
    "\t\t- Concatenated word embeddings\n",
    "\t\t- Words/one-hot vectors\n",
    "\t\t- improvements over n-gram LM:\n",
    "\t\t\t- no sparsity problem\n",
    "\t\t\t- don't need to store all observed n-grams\n",
    "\t\t- remaining problems:\n",
    "\t\t\t- fixed window is too small\n",
    "\t\t\t- enlarging window enlarges W\n",
    "\t\t\t- window can never be large enough\n",
    "\t\t\t- x^(1) and x^(2) are multiplied by completely different weights in W. no symmetry in how the inputs are processed.\n",
    "\t\t- --> we need a neural architecture that can process any length input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef913b5",
   "metadata": {},
   "source": [
    "**Recurrent Neural Networks (RNN)**\n",
    "- idea: apply the same weights W repeatedly and update parameters\n",
    "\n",
    "- a simple RNN LM:\n",
    "    - words/one-hot vectors --> embeddings --> hidden states (W matrix, add bias term, passing through non-linearity func)\n",
    "    - hidden states store memory of everything that's be seen so far\n",
    "    - repeat for each layer!\n",
    "    - then output\n",
    "\n",
    "- advantages:\n",
    "    - can process any length input\n",
    "    - computation for step t can use information from many steps back (in theory)\n",
    "    - model size doesn't increase for longer input context\n",
    "    - same weights applied on every timestep, so there is a symmetry in how inputs are processed\n",
    "\n",
    "- disadvantages:\n",
    "    - recurrent computation is slow\n",
    "    - in practive, difficult to access information from many steps back\n",
    "\n",
    "- training an RNN LM\n",
    "    - get a big corpus of text which is a sequence of words\n",
    "    - feed into RNN-LM --> compute output distribution for every step t (predict prob dist of every word, given words so far)\n",
    "    - loss function on step t is cross-entropy between predicted prob dist and the true next word (one hot)\n",
    "    - average this to get overall loss for entire training set\n",
    "    - (**teacher forcing** method)\n",
    "\n",
    "- backpropagation for RNNs\n",
    "    - what's the derivative of the loss (gradient) wrt the repreated weight matrix W_h?\n",
    "        - dJ^(t) / dW_h = sum of the gradient wrt each time it appears (multivariable chain rule)\n",
    "    - calculate this via backpropogation through time\n",
    "        - in practice, often truncated after 20 timesteps for training efficiency reasons\n",
    "\n",
    "- generating with an RNN LM (\"generating **roll outs**\")\n",
    "    - like n-gram LM, can use an RNN LM to generate text by repeated sampling\n",
    "    - sampled output becomes next step's input\n",
    "    - &lt;s&gt; = start of sequence\n",
    "    - &lt;s&gt; = end of sequence\n",
    "    - ex. chatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe836f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
