{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7851a93",
   "metadata": {},
   "source": [
    "**Lecture 1 - Intro and Word Vectors**\n",
    "\n",
    "- human language and word meaning\n",
    "- word2vec introduction\n",
    "- word2vec objective function gradients\n",
    "- optimization basics\n",
    "- looking at word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77223915",
   "metadata": {},
   "source": [
    "**Human Language**\n",
    "- langugage is always changing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774302f",
   "metadata": {},
   "source": [
    "**Word Meaning**\n",
    "\n",
    "- signifier (symbol) --> signified (idea or thing) = denotational semantics\n",
    "\n",
    "- WordNet: thesaurus containing lists of synonym sets and hypernyms\n",
    "    - problems:\n",
    "        - a useful resource but missing nuance\n",
    "        - missing new meanings of words\n",
    "        - subjective\n",
    "        - requires human labor to create and adapt\n",
    "        - can't be used to accurately compute word similarity\n",
    "\n",
    "- representing words as discrete symbols (one-hot encoding)\n",
    "    - problems:\n",
    "        - no natural notion of similarity\n",
    "    - solution:\n",
    "        - could try to use WordNet but fails badly (incompleteness)\n",
    "        - instead, learn to encode similarity in the vectors themselves\n",
    "\n",
    "- representing words by their context\n",
    "    - \"distributional semantics\": a word's meaning is given by the words that frequently appear close-by (context)\n",
    "\n",
    "- word vectors / word embeddings / neural word representations\n",
    "    - build a dense vector for each word\n",
    "    - chosen so that it is similar to vectors of words that appear in similar contexts\n",
    "    - measuring similarity as the vector dot (scalar) product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db7970",
   "metadata": {},
   "source": [
    "**Word2Vec**\n",
    "- framework for learning word vectors (2013)\n",
    "- large corpus of text\n",
    "- every word in a fixed vocab is represented by a vector\n",
    "- go through each position t in tht ext, which has a center word c and context words o\n",
    "- use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice cersa)\n",
    "    - predict conext words within a window of fixed size m, given venter word w_t (likelihood = product of probabilities)\n",
    "    - objective function is the average negative log likelihood (minimize obj func, maximize predictive accuracy)\n",
    "- keep adjusting the word vectors to maximize this probability\n",
    "    - how?\n",
    "    - we will use 2 vectors per word w\n",
    "        - v_w when w is center word\n",
    "        - u_w when w is context word\n",
    "    - P(o|c) = exp(u_o * v_c) / sum( exp(u_w * v_c) )\n",
    "        - 1. u_o * v_c dot product compares similarity of o and c (larger dot product = larger probability)\n",
    "        - 2. exponentiation makes anything positive\n",
    "        - 3. (denominator) normalize over entire vocabulary to give probability distribution\n",
    "    - this is an example of the **softmax function**\n",
    "        - maps arbitraty values x_i to a probability dist p_i\n",
    "        - \"max\" bc amplifies prob of largest x_i\n",
    "        - \"soft\" bc still assigns some prob to smaller x_i\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3547b",
   "metadata": {},
   "source": [
    "**Model Training: Optimize value of parameters to minimize loss**\n",
    "\n",
    "- gradually adjust parameters to minimize loss\n",
    "- d-dimensional vectors\n",
    "- V-many words\n",
    "- _theta_ representes all the model parameters\n",
    "- every word has 2 vectors\n",
    "    - one vector when they're the center word\n",
    "    - one vector when they're the outside word\n",
    "- optimize by walking down the gradient --> we compute ALL vector gradients\n",
    "    - partial derivatives of obj func with respect to vector of center word v_c"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
