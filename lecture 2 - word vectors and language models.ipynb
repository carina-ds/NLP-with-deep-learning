{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558ca0c3",
   "metadata": {},
   "source": [
    "**Lecture 2 - Word Vectors, Word Senses, and Neural Network Classifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805f22b",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "- for current value of _theta_, calculate gradient of cost function, then take small step in direction of negative gradient. repeat.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    theta_grad = evaluate_gradient(J, corpus, theta)\n",
    "    theta = theta - alpha * theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6ae44",
   "metadata": {},
   "source": [
    "- problem: function of ALL windoes in the corpus so very expensive to compute\n",
    "    - \"stochastic gradient descent\" (SGD)\n",
    "        - repeatedly sample windows, and update after each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    window = sample_window(corpus)\n",
    "    theta_grad = evaluate_gradient(J, window, theta)\n",
    "    theta = theta - alpha * theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365e622",
   "metadata": {},
   "source": [
    "**Word2Vec**\n",
    "- **bag of words model:** \n",
    "    - makes the same predictions at each position\n",
    "    - we want a model that gives a reasonably high probabiltiy estimate to ALL words that occur in the context\n",
    "\n",
    "- why 2 vectors?\n",
    "    - easier optimization. average both at the end\n",
    "    - but can implement the algorithm with just one vector per word...and it helps a bit\n",
    "\n",
    "- 2 model variants:\n",
    "    - 1. **skip-grams (SG)** - predict context words (position independent) given center word\n",
    "    - 2. **continuous bag of words (CBOW)** - predict center word from (bag of) context words\n",
    "\n",
    "- loss functions for training:\n",
    "    - 1. **naive softmax** (simple but expensive loss function, when many output classes)\n",
    "    - 2. more optimized variants like **hierarchical softmax**\n",
    "    - 3. **negative sampling**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737af7a4",
   "metadata": {},
   "source": [
    "**skip-gram model with negative sampling**\n",
    "\n",
    "- train binary logistic regressions to differentiate:\n",
    "    - a true pair (center word and a word in its context window) vs.\n",
    "    - several \"noise\" pairs (the center word paired with a random word)\n",
    "- we take K negative samples (using word probabilities *)\n",
    "- maximize probability of real outside word; minimize probability of random words\n",
    "    - minimize J_neg_samp(u_o, v_c, U) = -log_sigmoid_(u_o_real * v_c) - SUM for k sampled indicies (log_sigmoid_(-u_k_real_, v_c))\n",
    "        - note: **logistic/sigmoid function:** rather than softmax\n",
    "- sample with unigram probability per word (and raise them to the 3/4) \n",
    "    - reflects word frequency\n",
    "    - upping the probability of less frequent words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b2c4d",
   "metadata": {},
   "source": [
    "**Co-occurrence**\n",
    "\n",
    "- why not capture co-occurrence counts directly?\n",
    "    - instead of iterating through the whole corpus\n",
    "    - buildling co-corrence matrix X\n",
    "        - 2 options: window vs. full document\n",
    "        - window: similar to word2vec, use window around each word --> captures some syntactic and semantic information (\"word space\")\n",
    "        - word-document co-occurence matrix will give general topics (all sports terms will have similar entries) leading to \"Latent Semantic Analysis\" (\"document space\")\n",
    "\n",
    "- simple count co-occurent vectors\n",
    "    - vectors increase in size with vocabulary\n",
    "    - very high dimensional - require a lot of storage though sparse\n",
    "    - subsequent classificatio models have sparsity issues --> models are less robust\n",
    "\n",
    "- low-dimensional vectors\n",
    "    - idea: store \"most\" of the important information in a fixed, small number of dimensions: a dense vector\n",
    "    - usually 25-1000 dimentions, similar to word2vec\n",
    "\n",
    "\n",
    "- how to reduce the dimensionality?\n",
    "    - classic method: singluar value decomposition of co-occurence matrix X\n",
    "        - factorizes X into U * _SIGMA_ * V, where U and V are orthonormal (unit vectors and orthogonal)\n",
    "        - retain only k singular values, in order to generalize\n",
    "        - _X_hat_ is the best rank k approximation to X, in terms of least squares\n",
    "        - classic linear algebra results. expensive to compute for large matrices\n",
    "        - problem: running an SVD on raw counts doesn't work well\n",
    "        -   function words (the, he, has) are too frequent --> syntax has too much impact\n",
    "        - solution: \n",
    "            - scaling the counts in the cells can help A LOT\n",
    "                - log the frequencies\n",
    "                - min(X,t), with t approx. 100\n",
    "                - ignore the function words\n",
    "            - ramped windows that count closer words more than further away words\n",
    "            - use pearson correlations instead of counts, then set negative values to 0\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680a048",
   "metadata": {},
   "source": [
    "**Encoding meaning components in vector differences**\n",
    "\n",
    "- how can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?\n",
    "    - **log-bilinear model with vector differences:**\n",
    "        - w_i . w_j = log[P(i|j)]\n",
    "        - w_x . (w_a - w_b) = log[P(x|a) / P(x|b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2b356",
   "metadata": {},
   "source": [
    "**How to evaluate word vectors?**\n",
    "\n",
    "- a general concept of evaluation in NLP: intrinsic vs. extrinsic\n",
    "- **intrinsic:**\n",
    "    - evaluation on a specific/intermediate subtask\n",
    "    - fast to compute\n",
    "    - helps to understand that system\n",
    "    - not clear if really helpful unless correlation to real task is established\n",
    "- **extrinsic**\n",
    "    - evaluation on a real task\n",
    "    - can take a long time to compute accuracy\n",
    "    - unclear if the subsystem is the problem or its interaction or other subsystems\n",
    "    - if replacing exactly one subsystem with another improves accuracy --> winning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f75fd",
   "metadata": {},
   "source": [
    "**Intrinsic word vector evaluation**\n",
    "\n",
    "- word vector analogies\n",
    "    - evaluate word vecs by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions\n",
    "    - discarding the input words from the search !!!\n",
    "    - problem: what if the information is not linear?\n",
    "        - word vector distances vs. correlation with human judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414dfccf",
   "metadata": {},
   "source": [
    "**Extrinsic word vector evaluation**\n",
    "\n",
    "- **named entity recognition (NER):** identifying references to a person, organization, or location\n",
    "    - find and classify names in text by labeling word tokens\n",
    "\n",
    "    - simple NER: window classification using binary logistic classifier\n",
    "        - idea: classify each word in its context window of neighboring words\n",
    "        - train logistic classifier on hand-labeled data to classify center word (y/n) for each class based on a concatenation of word vectors in a window\n",
    "        - to classify all words: run classifier for each class on the vector centered on each word in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165be03",
   "metadata": {},
   "source": [
    "**Word senses and word sense ambiguity**\n",
    "\n",
    "- most words have lost of meanings\n",
    "    - especially common words\n",
    "    - especially words that have existed for a long time\n",
    "- does one vector capture all these meanings or do we have a mess?\n",
    "\n",
    "- improving word representations via global context and multiple word prototypes\n",
    "    - idea: cluster word windows around words, retrain with each word assigned to multiple different clusters (bank1 - money, bank2 - river, etc.)\n",
    "\n",
    "- linear algebraic structure of word senses\n",
    "    - different sense of a word reside in a linear superposition (weighted sum) in standard word embeddings like word2vec\n",
    "    - bc of ideas from sparse coding, you can separate out the senses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f452cee",
   "metadata": {},
   "source": [
    "**Neural classification**\n",
    "\n",
    "- typical softmax classifier \n",
    "    - learned parameters _theta_ are just elements of W (not input representation x, which has sparse symbolic features)\n",
    "    - problem: classifier gives linear decision boundary, which can be limiting\n",
    "\n",
    "- **neural network classifier**:\n",
    "    - we learn both W and (distributed) representations for words\n",
    "    - word vectors x re-represent one-hot vectors, moving them around in an intermediate vector space, for easy classification with a (linear) softmax classifier\n",
    "        - we have an embedding layer\n",
    "    - we use deep networks, more layers, that let us re-represent and compose our data multiple times giving a non-linear classifier\n",
    "    - 1. x (input) x = [x_museums x_in X_paris x_are x_amazing]\n",
    "    - 2. h = f(Wx + b), where f() is activation function\n",
    "    - 3. score s = u_transpose * h\n",
    "    - 4. predicted model prob of class = J_t(_theta_) = _sigmoid_(s) = 1 / (1 + e^-s)\n",
    "\n",
    "- training with cross entropy loss\n",
    "    - cross entropy = H(p,q) = -SUM( p(c) * log[q(c)] )\n",
    "    - since prob dist is 1 at right class and 0 everywhere else, loss function = negative log prob of the true class y_i = -log[p(y_i | x_i)]\n",
    "    - PyTorch: torch.nn..CrossEntropyLoss()\n",
    "\n",
    "- neural network = running several logistic regressions at the same time\n",
    "    - it is the final loss function that will direct what the intermediate hidden variables should be to predict targets for next layer well\n",
    "    - allows us to re-represent and compose our data multiple times and to learn a classifier that is highly non-linear in terms of the original inputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
