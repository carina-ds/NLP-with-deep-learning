{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5481676",
   "metadata": {},
   "source": [
    "**Lecture 4 - Dependency Parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ceaa",
   "metadata": {},
   "source": [
    "**Linguistic Structure**\n",
    "- 2 views:\n",
    "    - 1. **phrase structure** = constituency = context-free grammars (CFGs)\n",
    "        - organizes words into nested constituents\n",
    "    - 2. **dependency structure**\n",
    "        - shows which words depend on (modify, attach to, or arguments of) which other words\n",
    "\n",
    "- linear stream of words\n",
    "- problems\n",
    "    - prepositional phrase attachment ambiguity\n",
    "    - coordination scope ambiguity\n",
    "    - adjectival/adverbial modifier ambiguity\n",
    "    - verb phrase attachment ambiguity\n",
    "- solution: dependency paths help extract semantic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc7d79",
   "metadata": {},
   "source": [
    "**Dependency Grammar and Dependency Structure**\n",
    "\n",
    "- dependency syntax suggests that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (arrows) called dependencies\n",
    "    - arrows generally point from head to dependent\n",
    "- usually dependencies form a tree\n",
    "- usually add a fake ROOT so every word is a dependent of precisely 1 other node\n",
    "    - Ex. \"ROOT Dsicussion of the outstanding issues was completed.\"\n",
    "\n",
    "- the rise of annotated data & Universal Dependencies treebanks\n",
    "    - a lot slower and less useful than writing a grammar by hand\n",
    "    - but gives us:\n",
    "        - reusability of the labor\n",
    "        - broad coverage, not just a few intuitions\n",
    "        - frequencies and distributional information\n",
    "        - a way to evaluate NLP systems\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a0ae9",
   "metadata": {},
   "source": [
    "**Dependency Conditioning Preferences**\n",
    "\n",
    "- straightforward sources of information for dependency parsing:\n",
    "    - 1. **bilexical affinities** - the dependency is plausible\n",
    "    - 2. **dependency distance** - most dependencies are between nearby words\n",
    "    - 3. **intervening material** - dependencies rarely span intervening verbs or punctuation\n",
    "    - 4. **valency of heads** - how many dependents on which side are usual for a head?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c95c6d",
   "metadata": {},
   "source": [
    "**Dependency Parsing**\n",
    "\n",
    "- a sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of\n",
    "\n",
    "- usually some constraints:\n",
    "    - only one word is a dependent of ROOT\n",
    "    - don't want cycles A --> B, B --> A\n",
    "\n",
    "- makes the dependencies a tree\n",
    "- final issue: whether arrows can cross (be non-projective) or not\n",
    "\n",
    "- **projective parse** - there are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words\n",
    "    - dependencies corresponding to a CFG tree must be projective (by forming dependencies by taking 1 child of each category as head)\n",
    "    - most syntactic structure is projective, but dependency theory normally does allow non-projective structures to account for displaced constituents\n",
    "\n",
    "- methods:\n",
    "    - 1. dynamic programming\n",
    "    - 2. graph algorithms\n",
    "    - 3. contraint satisfaction\n",
    "    - 4. transition-based parsing or deterministic dependency parsing\n",
    "\n",
    "- **transition-based parsing** (greedy)\n",
    "    - simple form of a greedy discriminative dependency parser\n",
    "    - parser does a sequence of bottom-up actions\n",
    "        - like shift or reduce\n",
    "    - parser has:\n",
    "        - a stack _sigmoid_, written with top to the right, which starts with ROOT symbol\n",
    "        - a buffer _Beta_, written with top to the left, which starts with the input sentence\n",
    "        - a set of dependency arcs A, which starts off empty\n",
    "        - a set of actions (arc-building actions)\n",
    "    - ex. \"I ate fish\" (arc-standard transition-based parser)\n",
    "        - stack: [root], buffer: [I ate fish]\n",
    "        - shift --> stack: [root I ate] --> [root ate] = left arc, dependency = nsubj(ate --> I)\n",
    "        - shift --> stack: [root ate][fish] --> [root ate fish]\n",
    "        - stack: [root ate fish] --> [root ate], right arc, dependency = obj(ate --> fish)\n",
    "        - stack: [root ate] --> [root], right arc, dependency = root(root --> ate)\n",
    "        - finish\n",
    "\n",
    "- we have left to explain how we choose the next action --> machine learning\n",
    "    - each action is predicted by a discriminative classifier over each legal move\n",
    "        - max of 3 untyped choices\n",
    "        - features: top of stack word, POS; first in buffer word, POS; etc.\n",
    "    - sequence of transition predictions / ML operations\n",
    "    - there is no search (in the simplest form)\n",
    "        - but can do a beam search (slower but better)\n",
    "    - accuracy a bit below state of the art in dependency parsin but VERY fast linear time parsin, which high accuracy - great for parsing the web\n",
    "\n",
    "- evaluating dependency parsers via (unlabeled or labeled) dependency accuracy\n",
    "\n",
    "\n",
    "- why do we gain from a neural dependency parser?\n",
    "    - problem: categorical (indicator) features are sparse, incomplete, expensive to compute\n",
    "    - neural approach - learn a dense and compacy feature representation\n",
    "    - unlabeled attachement score (UAS) = head\n",
    "    - labeled attachement score (LAS) = head and label\n",
    "\n",
    "    - we can represent words as a d-dimensional dense vector (word embedding)\n",
    "        - similar words are expected to have close vectors\n",
    "    - part-of-speech tags (POS) and dependency labels are also represented as d-dimensional vectors\n",
    "        - smaller discrete sets also exhibit many semantical similarities\n",
    "            - NNS (plural noun) should be close to NN (singular noun)\n",
    "            - nummod (numerical modifier) shoudl be close to amod (adjective modifier)\n",
    "\n",
    "    - extract a set of tokens based on the stack/buffer positions\n",
    "    - concatenation of the vector representations --> neural representation of a configuration\n",
    "\n",
    "- **Neural Dependency Parser Model Architecture**\n",
    "    - a simple feed-forward neural network multi-class classifier\n",
    "    - operations: shift, left arc, right arc\n",
    "\n",
    "- deep learning classifiers are non-linear classifiers\n",
    "\n",
    "- **Graph-based dependency parsers**\n",
    "    - compute a score for every possible dependency for each word (ex. picking the head for \"big\")\n",
    "    - requires good contextual representations of each word token\n",
    "    - repeat the same process for each other word - find the best parse (MST algorithm - minimum spanning tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
