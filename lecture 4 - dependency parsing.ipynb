{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5481676",
   "metadata": {},
   "source": [
    "**Lecture 4 - Dependency Parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ceaa",
   "metadata": {},
   "source": [
    "**Linguistic Structure**\n",
    "- 2 views:\n",
    "    - 1. **phrase structure** = constituency = context-free grammars (CFGs)\n",
    "        - organizes words into nested constituents\n",
    "    - 2. **dependency structure**\n",
    "        - shows which words depend on (modify, attach to, or arguments of) which other words\n",
    "\n",
    "- linear stream of words\n",
    "- problems\n",
    "    - prepositional phrase attachment ambiguity\n",
    "    - coordination scope ambiguity\n",
    "    - adjectival/adverbial modifier ambiguity\n",
    "    - verb phrase attachment ambiguity\n",
    "- solution: dependency paths help extract semantic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc7d79",
   "metadata": {},
   "source": [
    "**Dependency Grammar and Dependency Structure**\n",
    "\n",
    "- dependency syntax suggests that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (arrows) called dependencies\n",
    "    - arrows generally point from head to dependent\n",
    "- usually dependencies form a tree\n",
    "- usually add a fake ROOT so every word is a dependent of precisely 1 other node\n",
    "    - Ex. \"ROOT Dsicussion of the outstanding issues was completed.\"\n",
    "\n",
    "- the rise of annotated data & Universal Dependencies treebanks\n",
    "    - a lot slower and less useful than writing a grammar by hand\n",
    "    - but gives us:\n",
    "        - reusability of the labor\n",
    "        - broad coverage, not just a few intuitions\n",
    "        - frequencies and distributional information\n",
    "        - a way to evaluate NLP systems\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a0ae9",
   "metadata": {},
   "source": [
    "**Dependency Conditioning Preferences**\n",
    "\n",
    "- straightforward sources of information for dependency parsing:\n",
    "    - 1. **bilexical affinities** - the dependency is plausible\n",
    "    - 2. **dependency distance** - most dependencies are between nearby words\n",
    "    - 3. **intervening material** - dependencies rarely span intervening verbs or punctuation\n",
    "    - 4. **valency of heads** - how many dependents on which side are usual for a head?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c95c6d",
   "metadata": {},
   "source": [
    "**Dependency Parsing**\n",
    "\n",
    "- a sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of\n",
    "\n",
    "- usually some constraints:\n",
    "    - only one word is a dependent of ROOT\n",
    "    - don't want cycles A --> B, B --> A\n",
    "\n",
    "- makes the dependencies a tree\n",
    "- final issue: whether arrows can cross (be non-projective) or not\n",
    "\n",
    "- **projective parse** - there are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words\n",
    "    - dependencies corresponding to a CFG tree must be projective (by forming dependencies by taking 1 child of each category as head)\n",
    "    - most syntactic structure is projective, but dependency theory normally does allow non-projective structures to account for displaced constituents\n",
    "\n",
    "- methods:\n",
    "    - 1. dynamic programming\n",
    "    - 2. graph algorithms\n",
    "    - 3. contraint satisfaction\n",
    "    - 4. transition-based parsing or deterministic dependency parsing\n",
    "\n",
    "- **transition-based parsing** (greedy)\n",
    "    - simple form of a greedy discriminative dependency parser\n",
    "    - parser does a sequence of bottom-up actions\n",
    "        - like shift or reduce\n",
    "    - parser has:\n",
    "        - a stack _sigmoid_, written with top to the right, which starts with ROOT symbol\n",
    "        - a buffer _Beta_, written with top to the left, which starts with the input sentence\n",
    "        - a set of dependency arcs A, which starts off empty\n",
    "        - a set of actions (arc-building actions)\n",
    "    - ex. \"I ate fish\" (arc-standard transition-based parser)\n",
    "        - stack: [root], buffer: [I ate fish]\n",
    "        - shift --> stack: [root I ate] --> [root ate] = left arc, dependency = nsubj(ate --> I)\n",
    "        - shift --> stack: [root ate][fish] --> [root ate fish]\n",
    "        - stack: [root ate fish] --> [root ate], right arc, dependency = obj(ate --> fish)\n",
    "        - stack: [root ate] --> [root], right arc, dependency = root(root --> ate)\n",
    "        - finish\n",
    "\n",
    "- we have left to explain how we choose the next action --> machine learning\n",
    "    - each action is predicted by a discriminative classifier over each legal move\n",
    "        - max of 3 untyped choices\n",
    "        - features: top of stack word, POS; first in buffer word, POS; etc.\n",
    "    - sequence of transition predictions / ML operations\n",
    "    - there is no search (in the simplest form)\n",
    "        - but can do a beam search (slower but better)\n",
    "    - accuracy a bit below state of the art in dependency parsin but VERY fast linear time parsin, which high accuracy - great for parsing the web\n",
    "\n",
    "- evaluating dependency parsers via (unlabeled or labeled) dependency accuracy\n",
    "\n",
    "\n",
    "- why do we gain from a neural dependency parser?\n",
    "    - problem: categorical (indicator) features are sparse, incomplete, expensive to compute\n",
    "    - neural approach - learn a dense and compacy feature representation\n",
    "    - unlabeled attachement score (UAS) = head\n",
    "    - labeled attachement score (LAS) = head and label\n",
    "\n",
    "    - we can represent words as a d-dimensional dense vector (word embedding)\n",
    "        - similar words are expected to have close vectors\n",
    "    - part-of-speech tags (POS) and dependency labels are also represented as d-dimensional vectors\n",
    "        - smaller discrete sets also exhibit many semantical similarities\n",
    "            - NNS (plural noun) should be close to NN (singular noun)\n",
    "            - nummod (numerical modifier) shoudl be close to amod (adjective modifier)\n",
    "\n",
    "    - extract a set of tokens based on the stack/buffer positions\n",
    "    - concatenation of the vector representations --> neural representation of a configuration\n",
    "\n",
    "- **Neural Dependency Parser Model Architecture**\n",
    "    - a simple feed-forward neural network multi-class classifier\n",
    "    - operations: shift, left arc, right arc\n",
    "\n",
    "- deep learning classifiers are non-linear classifiers\n",
    "\n",
    "- **Graph-based dependency parsers**\n",
    "    - compute a score for every possible dependency for each word (ex. picking the head for \"big\")\n",
    "    - requires good contextual representations of each word token\n",
    "    - repeat the same process for each other word - find the best parse (MST algorithm - minimum spanning tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

	• Modern neural networks are enormous (over 100 billion parameters)
	• We need regularization to prevent neural networks from overfitting
		○ Now regularization produces models that generalize well
		○ But we do not care that we overfit training data
	• Dropout
		○ When training, for each data point each time, randomly set input to 0 with probability p (dropout ratio, often p = 0.5 except p = 0.15 for input layer) via dropout mask
		○ Why it works
			§ Knows that some features are missing, so has be flexible in learning
			§ Prevents feature co-adaptation = good regularization
		○ In a single layer - middle ground between Naïve Bayes (all feature weights set independently) and logistic regression models (weights are set in the context of all others)
			§ A form of model bagging like an ensemble model
			§ Usually thought of as strong, feature-dependent regularizer
	• Vectorization
		○ Looping over word vectors vs. concatenating them all into one large matrix and then multiplying the softmax weights with that matrix
			§ From numpy import random
			§ N = 500 # number of windows to classify
			§ D = 300 # dimensionality of each window
			§ C = 5 # number of classes
			§ W = random.rand(C,d)
			§ Wordvectors_list = [random.rand(d,1) for I in range(N)]
			§ Wordvectors_one_matrix = random.rand(d,N)
		○ Always try to use vectors and matrices instead of for loops
		○ Use vector operations for your mask
	• Parameter initialization
		○ Must initialize weights to small random values (not zero matrices)
		○ To avoid symmetries that prevent learning/specialization
	• Optimizers
		○ Getting SGD rights is very dependent on getting the scales right for step size and learning rate
		○ For more complex nets, try more sophisticated "adaptive" optimizers that scale the adjustment to individual parameters by an accumulated gradient
			§ These models give differential per-parameter learning rates
				□ Adagrad - simplest, but tends to stall early
				□ RMSprop
				□ Adam - usually being with this
				□ AdamW
				□ NAdamW - can be better with word vectors (W) and for speed (Nesterov acceleration)
			§ Start them with an initial learning rate, around 0.001 - many have other hyperparameters
	• Language modeling
		○ The task of predicting the next word
		○ Given a sequence of words, compute the probability distribution of the next word, where next word can be any word in the vocabulary V
		○ A system that assigns a probability to a piece of text
	• n-gram Language Models
		○ How to learn a language model
			§ Pre-deep learning - learn an n-gram language model
				□  n-gram = chunk of n consecutive words (we are trying to predict)
				□ Unigram, bigram, trigram, four-gram
				□ Idea: collect statistics about how frequent different n-grams are and use these to predict next word
				□ we make a Markov assumption: x_t+1 depends only on the preceding n-1 words
					® Get n-gram and (n-1)-gram probabilities by counting them in some large corpus of text (statistical approximation)
		○ Ex. Learning a 4-gram Language Model
			§ "students opened their ____"
			§ P(w | students opened their) = count(students opened their w) / count(students opened their)
		○ Sparsity problems
			§ Problem 1: what if "students opened their w" never occurred in data? Then w has probability 0.
				□ Solution - add small _delta_ to the count for every w in V ("smoothing")
			§ Problem 2: what if "students opened their" never occurred in data? Then can't calculate probability for any w
				□ Solution - just condition on "opened their" instead ("backoff")
			§ 5-grams are the largest people really use
		○ Storage problems
			§ Need to store count for all n-grams you saw in the corpus
			§ Increasing n or increasing corpus increases model size
		○ In practice
			§ You can built a simple trigram language model over a 1.7 million word corpus (Reuters) in a few seconds on your laptop (nlpforhackers.io/language-models/
		○ Generating text
			§ Incoherent. We need to consider more than 3 words at a time in we want to model language well
			§ But increasing n worsens sparsity problem, and increases model size
				□ Solution - neural language model
	• Neural Language Models
		○ How to build a neural language model?
			§ Window-based neural model
				□ Output distribution
				□ Hidden layer
				□ Concatenated word embeddings
Words/one-hot vectors
